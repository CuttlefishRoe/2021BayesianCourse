# 决策分析（Decision Analysis）

## 一、背景：

早在1738年，伯努利就已提出决策分析中的效用概念。从1763年贝叶斯发表条件概率起就出现统计推断理论的萌芽。1815年拉普拉斯又将它推向一个新的阶段。统计推断理论实际上是在风险情况下的决策理论。1931年，拉姆齐基于效用和主观概率两个基本概念来研究决策理论。1944年 J.von诺伊曼和O.莫根施特恩在著名的《竞赛理论与经济行为》一书中，独立地研究了在不确定情况下进行决策所用的近代效用理论。沃尔德在1950年提出的统计决策函数是决策理论的又一重要进展。1954年萨维奇为决策方法提供了公理系统和严格的哲学基础。60年代初期，美国哈佛商学院开始运用统计决策理论解决商业问题，并定名为应用统计决策理论。1966年，美国霍华德首先应用决策分析这个名词。后来，决策分析又有许多新的发展，并广泛吸取有关的决策方法，从而形成一个内容广泛、实用性很强的学科分支。现代决策分析的发展动向是研究人们决策的行为思想方面和研制与计算机结合的决策支持系统等。

## 二、定义及基本方法：

### 2.1. 决策分析的定义

决策分析一般指从若干可能的方案中通过决策分析技术，如期望值法或决策树法等，选择其一的决策过程的定量分析方法。决策分析本质上比统计推断更复杂，因为它设计对决策的优化以及对不确定性的平均。

巴伐利亚决策分析的数学定义如下：
1.	列举所有可能的决定和结果。
2.	确定每个决策选项d的x的概率分布。用贝叶斯术语来说，这是条件后验分布，p(x|d)。在决策分析框架中，决策d不具有概率分布。所以我们不能说p(d)或p(x)；所有的概率都必须以d为条件。
3.	定义一个效用函数U(x)，将结果映射到实数上。在简单的问题中，效用可能与单个连续的收益x相一致，例如寿命或净利润。如果结果x有多个属性，效用函数必须权衡不同的goods。
4.	计算期望效用E(U(x)|d)作为决策d的函数，并选择期望效用最高的决策。在决策树中，可能会有两个或更多的决策，期望效用必须在每个决策点计算，条件是在该点之前的所有可用信息。

一个完整的决策分析包括所有这四个步骤。

### 2.2. 决策分析在不同情况下的方法
1.	确定性情况：每一个方案引起一个、而且只有一个结局。当方案个数较少时可以用穷举法，当方案个数较多时可以用一般最优化方法。
2.	随机性情况：也称风险性情况，即由一个方案可能引起几个结局中的一个，但各种结局以一定的概率发生。通常在能用某种估算概率的方法时，就可使用随机性决策，例如决策树的方法。
3.	不确定性情况:一个方案可能引起几个结局中的某一个结局,但各种结局的发生概率未知。这时可使用不确定型决策，例如拉普拉斯准则、乐观准则、悲观准则、遗憾准则等来取舍方案。
4.	多目标情况：由一个方案同时引起多个结局，它们分别属于不同属性或所追求的不同目标。这时一般采用多目标决策方法。例如化多为少的方法、分层序列法、直接找所有非劣解的方法等。
5.	多人决策情况：在同一个方案内有多个决策者，他们的利益不同，对方案结局的评价也不同。这时采用对策论、冲突分析、群决策等方法。

除上述各种方法外，还有对结局评价等有模糊性时采用的模糊决策方法和决策分析阶段序贯进行时所采用的序贯决策方法等。

## 三、两种决策分析方法

### 3.1. 决策树

决策树是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法，决策树法有利于决策人员使决策问题形象比，可把各种可以更换的方案、可能出现的状态、可能性大小及产生的后果等，简单地绘制在一张图上，以便计算、研究与分析，同时还可以随时补充和不确定型情况下的决策分析。构建决策树的过程中常用的指标是信息增益，还有Gini值和信息增益比,下面介绍这几个指标。

#### 3.1.1. 指标计算公式

对于待划分的数据集D，其划分前样本集合D的熵（总体的经验熵）是一定的，但是根据某个特征X划分之后的熵（X条件经验熵）是不定的，X条件经验熵越小说明使用此特征划分得到的子集的不确定性越小（也就是纯度越高），因此$ 总体经验熵 -  X条件经验熵 $差异越大，说明使用当前特征划分数据集D的话，其纯度上升的更快。而我们在构建最优的决策树的时候总希望能更快速到达纯度更高的集合，这一点可以参考优化算法中的梯度下降算法，每一步沿着负梯度方法最小化损失函数的原因就是负梯度方向是函数值减小最快的方向。同理：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。

由上我们知道计算信息增益（ID3算法）首先需要计算出总体的经验熵和条件经验熵，假设样本D有n个类别（特征），每个类别（特征）的概率是$ \frac{|C_n|}{D} $，其中$ |C_n| $表示类别（特征）n的样本个数，|D|表示样本总数，那么总体的经验熵计算公式如下：$$ H(D) = -\sum_{n=1}^N{\frac{|C_n|}{|D|}\log_2\frac{|C_n|}{|D|}} $$

X条件经验熵计算公式如下：$$ H(D|X) = \sum_{i=1}^n{p_iH(D|X=x_i)} $$

则X的信息增益计算Gain(X)公式为：$$ Gain(D，X) = H(D) - H(D|X) $$

使用信息增益法（ID）存在信息增益偏向取值较多的特征的缺点，因为当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，信息增益比较偏向取值较多的特征。

因此当样本D中特征的取值较多时，可以使用信息增益比（C4.5算法）作为指标，信息增益比的本质是在信息增益的基础之上乘上一个惩罚参数，当特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大，因此其计算公式为：$$ 信息增益比 = 信息增益 * 惩罚参数 $$

惩罚参数计算公式：$$ 惩罚参数 = \frac{1}{H_X (D)} = \frac{1}{-\sum_{i=1}^n{\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}}} $$
其中的$ H_X(D) $是对于样本集合D，将当前特征X作为随机变量（取值是特征X的各个特征值），求得的经验熵。

最终得到X的信息增益比：$$ g_R (D,X) = \frac{Gain(D,X)}{H_X (D)} $$

信息增益比偏向取值较少的特征，因为当特征取值较少时$ H_X(D) $的值较小，因此其倒数较大，因而信息增益比比较大，因而偏向取值较少的特征。所以在使用信息增益比时并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

除了信息增益和信息增益比这两个指标，常用的指标还有基尼指数（CART算法），基尼指数表示在样本集合中一个随机选中的样本被分错的概率，基尼指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯，即$$ 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率 $$

数学公式表示为：$$ Gini = \sum_{k=1}^K{p_k(1-p_k)} = 1 - \sum_{k=1}^K{p_k ^2} $$

其中$ p_k $表示选中的样本属于k类别的概率，则这个样本被分错的概率是$ (1-p_k) $,样本集合中有K个类别，一个随机选中的样本可以属于这k个类别中的任意一个，因而对类别就加和,当样本为二分类样本时，Gini(P) = 2p(1-p)




























